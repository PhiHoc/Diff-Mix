{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def beta_sampling(num_samples, beta=5):\n",
    "\n",
    "    if num_samples < 1:\n",
    "        raise ValueError(\"Number of samples must be greater than or equal to 1\")\n",
    "    # beta distribution with alpha = 1, beta = 1, clamp between 0.1 to 0.9\n",
    "# Define the desired range\n",
    "    min_value = 0.1\n",
    "    max_value = 0.9\n",
    "\n",
    "    # Generate and clamp the values\n",
    "    sampled_values = [np.clip(np.round(np.random.beta(beta, beta), 3),a_min=min_value,a_max=max_value) for _ in range(num_samples)]\n",
    "    \n",
    "    return sampled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, vit_b_16,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_aug.datasets.cub import CUBBirdHugDatasetForT2I,CUBBirdHugImbalanceDataset\n",
    "from semantic_aug.datasets.aircraft import AircraftHugDatasetForT2I\n",
    "from semantic_aug.datasets.flower import FlowersDatasetForT2I,FlowersImbalanceDataset\n",
    "from semantic_aug.datasets.car import CarHugDatasetForT2I\n",
    "from semantic_aug.datasets.chest import ChestHugDatasetForT2I\n",
    "from semantic_aug.datasets.pet import PetHugDatasetForT2I, PetHugDataset\n",
    "from semantic_aug.datasets.food import FoodHugDatasetForT2I\n",
    "from datasets import load_dataset\n",
    "# ds = load_dataset('/home/zhicai/.cache/huggingface/datasets/pcuenq___oxford-pets/default-7fcafd63f4da1c6c', split='train')\n",
    "# ds = PetHugDataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size filtered from 4070 to 1532 with imbalance factor 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1532"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FlowersImbalanceDataset(imbalance_factor=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cub train 5994 200\n",
      "cub val 5794 200\n",
      "flower train 4070 102\n",
      "flower val 4119 102\n",
      "aircraft train 3334 100\n",
      "aircraft val 3333 100\n",
      "car train 8144 196\n",
      "car val 8041 196\n",
      "dog train 12000 120\n",
      "dog val 8580 120\n"
     ]
    }
   ],
   "source": [
    "from utils import DATASET_NAME_MAPPING\n",
    "\n",
    "for dataset in ['cub','flower','aircraft','car','dog']:\n",
    "    for split in ['train','val']:\n",
    "        ds = DATASET_NAME_MAPPING[dataset](split=split)\n",
    "        print(dataset,split,len(ds),ds.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目录 outputs/result/main_result_10shot/cub11140316_base_100_sgd_224_resnet50_lr0.05_['mixup1.0_10shot']_2020 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131520_base_100_sgd_224_resnet50_lr0.1_['realmixup0.1_10shot']_2022 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/cub11140316_base_100_sgd_224_resnet50_lr0.05_['mixup1.0_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131521_base_100_sgd_224_resnet50_lr0.1_['mixup0.3_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131522_base_100_sgd_224_resnet50_lr0.1_['mixup0.9_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131521_base_100_sgd_224_resnet50_lr0.1_['mixup0.1_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131522_base_100_sgd_224_resnet50_lr0.1_['mixup1.0_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131520_base_100_sgd_224_resnet50_lr0.1_['realmixup0.5_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131521_base_100_sgd_224_resnet50_lr0.1_['mixup0.7_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131521_base_100_sgd_224_resnet50_lr0.1_['realmixup1.0_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131521_base_100_sgd_224_resnet50_lr0.1_['mixup0.5_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/cub11140316_base_100_sgd_224_resnet50_lr0.05_['mixup1.0_10shot']_2022 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131520_base_100_sgd_224_resnet50_lr0.1_['realmixup0.1_10shot']_2020 已被强制删除，因为没有找到 acc_eval_* 文件。\n",
      "目录 outputs/result/main_result_10shot/aircraft11131520_base_100_sgd_224_resnet50_lr0.1_['realmixup0.1_10shot']_2021 已被强制删除，因为没有找到 acc_eval_* 文件。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "target_dir = 'outputs/result/main_result_10shot'\n",
    "for dir_name in os.listdir(target_dir):\n",
    "    directory = os.path.join(target_dir, dir_name) \n",
    "    if os.path.exists(directory) and os.path.isdir(directory):\n",
    "        if not any(filename.startswith(\"acc_eval_\") for filename in os.listdir(directory)):\n",
    "            if not any(filename.startswith(\"max_acc\") for filename in os.listdir(directory)):\n",
    "            # 强制删除目录及其内容\n",
    "                shutil.rmtree(directory)\n",
    "                print(f\"目录 {directory} 已被强制删除，因为没有找到 acc_eval_* 文件。\")\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        print(f\"目录 {directory} 不存在。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:8890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:8890\"\n",
    "HUB_LOCAL_DIR = 'jonathancui/oxford-pets'\n",
    "ds =  load_dataset(HUB_LOCAL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 23:31:41.381363: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'outputs/aug_samples_1shot/cub/dreambooth-lora-generation-Multi5' is valid with 1002 files.\n",
      "cub\t\t:gen_1shot\n",
      "Directory 'outputs/aug_samples_5shot/cub/dreambooth-lora-generation-Multi5_db_ti_latest_5shot' is valid with 1002 files.\n",
      "cub\t\t:gen_5shot\n",
      "Directory 'outputs/aug_samples_10shot/cub/dreambooth-lora-generation-Multi5_db_ti_latest_10shot' is valid with 10002 files.\n",
      "cub\t\t:gen_10shot\n",
      "Directory 'outputs/aug_samples_1shot/cub/real-generation-Multi5' is valid with 334 files.\n",
      "cub\t\t:realgen_1shot\n",
      "Directory 'outputs/aug_samples_5shot/cub/real-generation-Multi5' is valid with 5002 files.\n",
      "cub\t\t:realgen_5shot\n",
      "Directory 'outputs/aug_samples_1shot/aircraft/dreambooth-lora-generation-Multi5' is valid with 502 files.\n",
      "aircraft\t\t:gen_1shot\n",
      "Directory 'outputs/aug_samples_5shot/aircraft/dreambooth-lora-generation-Multi5_db_ti_latest_5shot' is valid with 2502 files.\n",
      "aircraft\t\t:gen_5shot\n",
      "Directory 'outputs/aug_samples_10shot/aircraft/dreambooth-lora-generation-Multi5_db_ti_latest_10shot' is valid with 5002 files.\n",
      "aircraft\t\t:gen_10shot\n",
      "Directory 'outputs/aug_samples_1shot/aircraft/real-generation-Multi5' is valid with 502 files.\n",
      "aircraft\t\t:realgen_1shot\n",
      "Directory 'outputs/aug_samples_5shot/aircraft/real-generation-Multi5' is valid with 2501 files.\n",
      "aircraft\t\t:realgen_5shot\n",
      "Directory 'outputs/aug_samples_10shot/aircraft/real-generation-Multi5' is valid with 5002 files.\n",
      "aircraft\t\t:realgen_10shot\n"
     ]
    }
   ],
   "source": [
    "from semantic_aug.datasets.cub import CUBBirdHugImbalanceDataset\n",
    "from semantic_aug.datasets.flower import FlowersImbalanceDataset\n",
    "from utils import parse_synthetic_dir\n",
    "import torch\n",
    "invalid = []\n",
    "# for dataset in ['cub','aircraft']:\n",
    "#     for strength in [0.1,0.3,0.5,0.7,0.9,1.0]:\n",
    "#         for strategy in ['mixup', 'aug','realmixup','realaug']:\n",
    "#             for shot in [1,5,10]:\n",
    "#                 try:\n",
    "#                     synthetic_dir = parse_synthetic_dir(dataset,f'{strategy}{strength}_{shot}shot' )\n",
    "#                 except:\n",
    "#                     invalid.append(f'{dataset}_{strategy}{strength}_{shot}shot')         \n",
    "#                     continue\n",
    "for dataset in ['cub','aircraft']:\n",
    "    for strategy in ['gen', 'realgen']:\n",
    "        for shot in [1,5,10]:\n",
    "            try:\n",
    "                synthetic_dir = parse_synthetic_dir(dataset,f'{strategy}_{shot}shot' )\n",
    "            except:\n",
    "                invalid.append(f'{dataset}_{strategy}_{shot}shot')         \n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cub_realgen_10shot']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.get_cls_num_list()[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from utils import parse_synthetic_dir\n",
    "import random\n",
    "\n",
    "for dataset in ['cub', 'flower']:\n",
    "    for name in ['mixup0.7_imb0.01','mixup0.7_imb0.05','mixup0.7_imb0.1']:\n",
    "        try:\n",
    "            parse_synthetic_dir(dataset, name)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{dataset} {name} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "path = '/data/zhicai/datasets/fgvc_datasets/CUB_200_2011/split/train/'\n",
    "\n",
    "IMAGE_EXTENSIONS = {'bmp', 'jpg', 'jpeg', 'pgm', 'png', 'ppm',\n",
    "                    'tif', 'tiff', 'webp'}\n",
    "path = pathlib.Path(path)\n",
    "sorted([file for file in path.glob('*') if file.suffix[1:] in IMAGE_EXTENSIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from semantic_aug.datasets.cub import CUBBirdHugDataset\n",
    "meta = pd.read_csv('outputs/aug_samples/cub/mixup_e100_uniform90000/meta.csv')\n",
    "ds=CUBBirdHugDataset()\n",
    "target_class_num=80\n",
    "selected_indexs=[]\n",
    "for source_name in ds.class_names:\n",
    "    target_classes = random.sample(ds.class_names, target_class_num)\n",
    "    indexs = meta[(meta['First Directory']==source_name) &(meta['Second Directory'].isin(target_classes))]\n",
    "    selected_indexs.append(indexs)\n",
    "\n",
    "meta2 = pd.concat(selected_indexs,axis=0)\n",
    "total_num = min(len(meta2),18000)\n",
    "idxs=random.sample(range(len(meta2)),total_num)\n",
    "meta2 = meta2.iloc[idxs]\n",
    "meta2.reset_index(drop=True,inplace=True)\n",
    "len(meta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
